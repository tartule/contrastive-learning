{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32b7a181",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install tensorflow-addons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c020e47d",
   "metadata": {},
   "source": [
    "I want to optimize the following parameters :\n",
    "\n",
    "\n",
    "- learning rate\n",
    "- optimizer (SGD,adam)\n",
    "- temperature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6916bc",
   "metadata": {},
   "source": [
    "# set up the parameters needed for the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b957a154",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorboard.plugins.hparams import api as hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9d5ee93",
   "metadata": {},
   "outputs": [],
   "source": [
    "HP_temperature = hp.HParam('temperature', hp.RealInterval(0.,0.2))\n",
    "HP_learning_rate = hp.HParam('learning_rate', hp.RealInterval(0., 0.2))\n",
    "HP_OPTIMIZER = hp.HParam('optimizer', hp.Discrete(['adam', 'sgd']))\n",
    "\n",
    "METRIC_ACCURACY = 'accuracy'\n",
    "\n",
    "\n",
    "with tf.summary.create_file_writer('logs/hparam_tuning').as_default():\n",
    "  hp.hparams_config(\n",
    "    hparams=[HP_temperature, HP_learning_rate, HP_OPTIMIZER],\n",
    "    metrics=[hp.Metric(METRIC_ACCURACY, display_name='Accuracy')],\n",
    "  )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0efa49c",
   "metadata": {},
   "source": [
    "# define the training of the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7797c2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "100dbd81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (50000, 32, 32, 3) - y_train shape: (50000, 1)\n",
      "x_test shape: (10000, 32, 32, 3) - y_test shape: (10000, 1)\n"
     ]
    }
   ],
   "source": [
    "num_classes = 10\n",
    "input_shape = (32, 32, 3)\n",
    "\n",
    "AUTO=tf.data.AUTOTUNE\n",
    "#learning_rate = 0.001\n",
    "batch_size = 265\n",
    "hidden_units = 512\n",
    "projection_units = 128\n",
    "num_epochs = 50\n",
    "dropout_rate = 0.5\n",
    "#temperature = 0.05\n",
    "\n",
    "\n",
    "\n",
    "# Load the train and test data splits\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "# Display shapes of train and test datasets\n",
    "print(f\"x_train shape: {x_train.shape} - y_train shape: {y_train.shape}\")\n",
    "print(f\"x_test shape: {x_test.shape} - y_test shape: {y_test.shape}\")\n",
    "\n",
    "test_dataset=(tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "    .shuffle(1024)\n",
    "    .batch(batch_size)\n",
    "    .prefetch(AUTO)\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f4ca8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = keras.Sequential(\n",
    "    [\n",
    "        layers.Normalization(),\n",
    "        layers.RandomFlip(\"horizontal\"),\n",
    "        layers.RandomRotation(0.02),\n",
    "        layers.RandomWidth(0.2),\n",
    "        layers.RandomHeight(0.2),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Setting the state of the normalization layer.\n",
    "data_augmentation.layers[0].adapt(x_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "272493ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_encoder(backbone=keras.applications.ResNet50V2(\n",
    "        include_top=False, weights=None, input_shape=input_shape, pooling=\"avg\"\n",
    "    )):\n",
    "    \n",
    "\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    augmented = data_augmentation(inputs)\n",
    "    outputs = backbone(augmented)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs, name=\"cifar10-encoder\")\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_classifier(encoder, trainable=True):\n",
    "    learning_rate=0.001\n",
    "    for layer in encoder.layers:\n",
    "        layer.trainable = trainable\n",
    "\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    features = encoder(inputs)\n",
    "    features = layers.Dropout(dropout_rate)(features)\n",
    "    features = layers.Dense(hidden_units, activation=\"relu\")(features)\n",
    "    features = layers.Dropout(dropout_rate)(features)\n",
    "    outputs = layers.Dense(num_classes, activation=\"softmax\")(features)\n",
    "\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs, name=\"cifar10-classifier\")\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate),\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "        metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "class SupervisedContrastiveLoss(keras.losses.Loss):\n",
    "    def __init__(self, temperature=1, name=None):\n",
    "        super(SupervisedContrastiveLoss, self).__init__(name=name)\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def __call__(self, labels, feature_vectors, sample_weight=None):\n",
    "        # Normalize feature vectors\n",
    "        feature_vectors_normalized = tf.math.l2_normalize(feature_vectors, axis=1)\n",
    "        # Compute logits\n",
    "        logits = tf.divide(\n",
    "            tf.matmul(\n",
    "                feature_vectors_normalized, tf.transpose(feature_vectors_normalized)\n",
    "            ),\n",
    "            self.temperature,\n",
    "        )\n",
    "        return tfa.losses.npairs_loss(tf.squeeze(labels), logits)\n",
    "\n",
    "\n",
    "def add_projection_head(encoder):\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    features = encoder(inputs)\n",
    "    outputs = layers.Dense(projection_units, activation=\"relu\")(features)\n",
    "    model = keras.Model(\n",
    "        inputs=inputs, outputs=outputs, name=\"cifar-encoder_with_projection-head\"\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def apply_resnet_block(x,downsample,conv_by_block):\n",
    "    \n",
    "    depth_input=x.shape[-1]\n",
    "    \n",
    "    \n",
    "    if downsample:\n",
    "        depth=depth_input*2\n",
    "        skiped=layers.Conv2D(depth,1,strides=(2,2),activation=None)(x)#linear projection\n",
    "        x=layers.Conv2D(depth,3,strides=(2,2), activation='relu',padding=\"same\")(x)\n",
    "        x=layers.BatchNormalization()(x)\n",
    "    else:\n",
    "        depth=depth_input\n",
    "        skiped=x\n",
    "        x=layers.Conv2D(depth,3, activation='relu',padding=\"same\")(x)\n",
    "        x=layers.BatchNormalization()(x)\n",
    "        \n",
    "    for i in range(1,conv_by_block-1):\n",
    "        x=layers.Conv2D(depth,3, activation='relu',padding=\"same\")(x)\n",
    "        x=layers.BatchNormalization()(x)\n",
    "    \n",
    "    x=layers.Conv2D(depth,3,padding=\"same\")(x)#don't apply activation to the last \n",
    "         \n",
    "    x=skiped+x\n",
    "    x=layers.ReLU()(x)\n",
    "    x=layers.BatchNormalization()(x)\n",
    "   \n",
    "    return x\n",
    "\n",
    "def get_resnet_backbone(input_shape,hparams):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    assert hparams[\"conv_by_block\"]>=2\n",
    "    inputs=layers.Input((32,32,3))\n",
    "    x=layers.Conv2D(hparams[\"depth_first_convolution\"],7,strides=(2,2),activation='relu',padding=\"same\")(inputs)\n",
    "    x=layers.BatchNormalization()(x)\n",
    "    for block in range(1,hparams[\"number_of_block\"]+1):\n",
    "        x=apply_resnet_block(x,block in hparams[\"downsample_num\"],hparams[\"conv_by_block\"])\n",
    "       \n",
    "        \n",
    "    x=layers.Conv2D(hparams[\"output_dim\"],3, activation='relu',padding=\"same\")(x)\n",
    "    x=layers.BatchNormalization()(x)\n",
    "    if hparams[\"globalPoolingType\"]==\"Mean\":\n",
    "        \n",
    "        x=layers.GlobalAveragePooling2D()(x)\n",
    " \n",
    "    return tf.keras.Model(inputs,x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb95fd8c",
   "metadata": {},
   "source": [
    "## model definition with the api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "60b2267f",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_training=10\n",
    "learning_rate=10**np.random.uniform(-0.3,-3,number_of_training)\n",
    "temperature=np.random.uniform(0.01,0.2,number_of_training)\n",
    "optimizer=np.random.choice([\"adam\",\"SGD\"],number_of_training)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fcc0ff35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 {'temperature': 0.07167965229184195, 'optimizer': 'SGD', 'learning_rate': 0.49778680580027007}\n",
      "1 {'temperature': 0.143440051763222, 'optimizer': 'adam', 'learning_rate': 0.03191585744792269}\n",
      "2 {'temperature': 0.19814413672203127, 'optimizer': 'adam', 'learning_rate': 0.016089648642657405}\n",
      "3 {'temperature': 0.18783177697486292, 'optimizer': 'adam', 'learning_rate': 0.47137449329034486}\n",
      "4 {'temperature': 0.01632473454774216, 'optimizer': 'adam', 'learning_rate': 0.0012719336272152611}\n",
      "5 {'temperature': 0.07595381732611305, 'optimizer': 'SGD', 'learning_rate': 0.003135672683222365}\n",
      "6 {'temperature': 0.12355834498251328, 'optimizer': 'SGD', 'learning_rate': 0.15799313996496078}\n",
      "7 {'temperature': 0.014626659456859606, 'optimizer': 'adam', 'learning_rate': 0.0013602300641906559}\n",
      "8 {'temperature': 0.01960442658292879, 'optimizer': 'SGD', 'learning_rate': 0.4194842499443589}\n",
      "9 {'temperature': 0.14383485948403216, 'optimizer': 'adam', 'learning_rate': 0.1557032048305991}\n"
     ]
    }
   ],
   "source": [
    "config_to_test={i:{\"temperature\":temperature[i],\"optimizer\":optimizer[i],\"learning_rate\":learning_rate[i]} for i in range(number_of_training)}\n",
    "for n,hparams in config_to_test.items():\n",
    "    print(n,hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1dccc740",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.3010299956639812"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log10(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ae8919d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting trial: run-0\n",
      "{'temperature': 0.07167965229184195, 'optimizer': 'SGD', 'learning_rate': 0.49778680580027007}\n",
      "189/189 - 35s - loss: 6.0399 - 35s/epoch - 184ms/step\n",
      "Epoch 1/10\n",
      "189/189 [==============================] - 15s 57ms/step - loss: 3.2553 - sparse_categorical_accuracy: 0.2174\n",
      "Epoch 2/10\n",
      "189/189 [==============================] - 10s 55ms/step - loss: 2.0437 - sparse_categorical_accuracy: 0.2458\n",
      "Epoch 3/10\n",
      "189/189 [==============================] - 11s 56ms/step - loss: 1.9995 - sparse_categorical_accuracy: 0.2576\n",
      "Epoch 4/10\n",
      "189/189 [==============================] - 11s 58ms/step - loss: 1.9738 - sparse_categorical_accuracy: 0.2602\n",
      "Epoch 5/10\n",
      "189/189 [==============================] - 11s 56ms/step - loss: 1.9645 - sparse_categorical_accuracy: 0.2645\n",
      "Epoch 6/10\n",
      "189/189 [==============================] - 11s 56ms/step - loss: 1.9591 - sparse_categorical_accuracy: 0.2677\n",
      "Epoch 7/10\n",
      "189/189 [==============================] - 11s 58ms/step - loss: 1.9492 - sparse_categorical_accuracy: 0.2707\n",
      "Epoch 8/10\n",
      "189/189 [==============================] - 11s 56ms/step - loss: 1.9502 - sparse_categorical_accuracy: 0.2708\n",
      "Epoch 9/10\n",
      "189/189 [==============================] - 11s 57ms/step - loss: 1.9402 - sparse_categorical_accuracy: 0.2693\n",
      "Epoch 10/10\n",
      "189/189 [==============================] - 11s 59ms/step - loss: 1.9421 - sparse_categorical_accuracy: 0.2733\n",
      "313/313 [==============================] - 4s 11ms/step - loss: 1.8811 - sparse_categorical_accuracy: 0.2966\n",
      "--- Starting trial: run-1\n",
      "{'temperature': 0.143440051763222, 'optimizer': 'adam', 'learning_rate': 0.03191585744792269}\n",
      "189/189 - 36s - loss: 5.4869 - 36s/epoch - 189ms/step\n",
      "Epoch 1/10\n",
      "189/189 [==============================] - 15s 58ms/step - loss: 2.2829 - sparse_categorical_accuracy: 0.2305\n",
      "Epoch 2/10\n",
      "189/189 [==============================] - 11s 56ms/step - loss: 1.9254 - sparse_categorical_accuracy: 0.2471\n",
      "Epoch 3/10\n",
      "189/189 [==============================] - 11s 56ms/step - loss: 1.9076 - sparse_categorical_accuracy: 0.2559\n",
      "Epoch 4/10\n",
      "189/189 [==============================] - 11s 56ms/step - loss: 1.9020 - sparse_categorical_accuracy: 0.2575\n",
      "Epoch 5/10\n",
      "189/189 [==============================] - 11s 57ms/step - loss: 1.8948 - sparse_categorical_accuracy: 0.2571\n",
      "Epoch 6/10\n",
      "189/189 [==============================] - 11s 56ms/step - loss: 1.8957 - sparse_categorical_accuracy: 0.2578\n",
      "Epoch 7/10\n",
      "189/189 [==============================] - 10s 54ms/step - loss: 1.8969 - sparse_categorical_accuracy: 0.2600\n",
      "Epoch 8/10\n",
      "189/189 [==============================] - 11s 56ms/step - loss: 1.8952 - sparse_categorical_accuracy: 0.2576\n",
      "Epoch 9/10\n",
      "189/189 [==============================] - 11s 56ms/step - loss: 1.8908 - sparse_categorical_accuracy: 0.2614\n",
      "Epoch 10/10\n",
      "189/189 [==============================] - 10s 55ms/step - loss: 1.8899 - sparse_categorical_accuracy: 0.2618\n",
      "313/313 [==============================] - 4s 11ms/step - loss: 1.8191 - sparse_categorical_accuracy: 0.2924\n",
      "--- Starting trial: run-2\n",
      "{'temperature': 0.19814413672203127, 'optimizer': 'adam', 'learning_rate': 0.016089648642657405}\n",
      "189/189 - 35s - loss: 5.4631 - 35s/epoch - 188ms/step\n",
      "Epoch 1/10\n",
      "189/189 [==============================] - 18s 77ms/step - loss: 2.6436 - sparse_categorical_accuracy: 0.2112\n",
      "Epoch 2/10\n",
      "189/189 [==============================] - 18s 94ms/step - loss: 1.9569 - sparse_categorical_accuracy: 0.2328\n",
      "Epoch 3/10\n",
      "189/189 [==============================] - 11s 57ms/step - loss: 1.9237 - sparse_categorical_accuracy: 0.2402\n",
      "Epoch 4/10\n",
      "189/189 [==============================] - 10s 55ms/step - loss: 1.9101 - sparse_categorical_accuracy: 0.2490\n",
      "Epoch 5/10\n",
      "189/189 [==============================] - 11s 57ms/step - loss: 1.9019 - sparse_categorical_accuracy: 0.2482\n",
      "Epoch 6/10\n",
      "189/189 [==============================] - 10s 54ms/step - loss: 1.9006 - sparse_categorical_accuracy: 0.2491\n",
      "Epoch 7/10\n",
      "189/189 [==============================] - 10s 55ms/step - loss: 1.8949 - sparse_categorical_accuracy: 0.2535\n",
      "Epoch 8/10\n",
      "189/189 [==============================] - 11s 56ms/step - loss: 1.8900 - sparse_categorical_accuracy: 0.2543\n",
      "Epoch 9/10\n",
      "189/189 [==============================] - 10s 55ms/step - loss: 1.8904 - sparse_categorical_accuracy: 0.2535\n",
      "Epoch 10/10\n",
      "189/189 [==============================] - 11s 56ms/step - loss: 1.8866 - sparse_categorical_accuracy: 0.2561\n",
      "313/313 [==============================] - 18s 11ms/step - loss: 1.8319 - sparse_categorical_accuracy: 0.2754\n",
      "--- Starting trial: run-3\n",
      "{'temperature': 0.18783177697486292, 'optimizer': 'adam', 'learning_rate': 0.47137449329034486}\n",
      "189/189 - 37s - loss: 5.5594 - 37s/epoch - 198ms/step\n",
      "Epoch 1/10\n",
      "189/189 [==============================] - 17s 56ms/step - loss: 6.0170 - sparse_categorical_accuracy: 0.0994\n",
      "Epoch 2/10\n",
      "189/189 [==============================] - 10s 55ms/step - loss: 2.3067 - sparse_categorical_accuracy: 0.0981\n",
      "Epoch 3/10\n",
      "189/189 [==============================] - 11s 57ms/step - loss: 2.3036 - sparse_categorical_accuracy: 0.0978\n",
      "313/313 [==============================] - 17s 11ms/step - loss: 2.3026 - sparse_categorical_accuracy: 0.1000\n",
      "--- Starting trial: run-4\n",
      "{'temperature': 0.01632473454774216, 'optimizer': 'adam', 'learning_rate': 0.0012719336272152611}\n",
      "189/189 - 38s - loss: 8.7643 - 38s/epoch - 199ms/step\n",
      "Epoch 1/10\n",
      "189/189 [==============================] - 17s 55ms/step - loss: 4.0451 - sparse_categorical_accuracy: 0.1220\n",
      "Epoch 2/10\n",
      "189/189 [==============================] - 11s 56ms/step - loss: 2.3283 - sparse_categorical_accuracy: 0.1301\n",
      "Epoch 3/10\n",
      "189/189 [==============================] - 10s 55ms/step - loss: 2.3061 - sparse_categorical_accuracy: 0.1324\n",
      "Epoch 4/10\n",
      "189/189 [==============================] - 11s 57ms/step - loss: 2.2931 - sparse_categorical_accuracy: 0.1380\n",
      "Epoch 5/10\n",
      "189/189 [==============================] - 11s 56ms/step - loss: 2.2837 - sparse_categorical_accuracy: 0.1393\n",
      "Epoch 6/10\n",
      "189/189 [==============================] - 11s 57ms/step - loss: 2.2849 - sparse_categorical_accuracy: 0.1405\n",
      "Epoch 7/10\n",
      "189/189 [==============================] - 11s 58ms/step - loss: 2.2777 - sparse_categorical_accuracy: 0.1414\n",
      "Epoch 8/10\n",
      "189/189 [==============================] - 11s 56ms/step - loss: 2.2732 - sparse_categorical_accuracy: 0.1455\n",
      "Epoch 9/10\n",
      "189/189 [==============================] - 11s 56ms/step - loss: 2.2730 - sparse_categorical_accuracy: 0.1477\n",
      "Epoch 10/10\n",
      "189/189 [==============================] - 11s 56ms/step - loss: 2.2721 - sparse_categorical_accuracy: 0.1460\n",
      "313/313 [==============================] - 17s 11ms/step - loss: 2.2412 - sparse_categorical_accuracy: 0.1575\n",
      "--- Starting trial: run-5\n",
      "{'temperature': 0.07595381732611305, 'optimizer': 'SGD', 'learning_rate': 0.003135672683222365}\n",
      "189/189 - 37s - loss: 6.3437 - 37s/epoch - 196ms/step\n",
      "Epoch 1/10\n",
      "189/189 [==============================] - 21s 76ms/step - loss: 2.0261 - sparse_categorical_accuracy: 0.2228\n",
      "Epoch 2/10\n",
      "189/189 [==============================] - 10s 55ms/step - loss: 1.9442 - sparse_categorical_accuracy: 0.2641\n",
      "Epoch 3/10\n",
      "189/189 [==============================] - 11s 56ms/step - loss: 1.9066 - sparse_categorical_accuracy: 0.2813\n",
      "Epoch 4/10\n",
      "189/189 [==============================] - 11s 57ms/step - loss: 1.8813 - sparse_categorical_accuracy: 0.2926\n",
      "Epoch 5/10\n",
      "189/189 [==============================] - 10s 54ms/step - loss: 1.8771 - sparse_categorical_accuracy: 0.2928\n",
      "Epoch 6/10\n",
      "189/189 [==============================] - 11s 57ms/step - loss: 1.8654 - sparse_categorical_accuracy: 0.2999\n",
      "Epoch 7/10\n",
      "189/189 [==============================] - 11s 57ms/step - loss: 1.8490 - sparse_categorical_accuracy: 0.3050\n",
      "Epoch 8/10\n",
      "189/189 [==============================] - 11s 56ms/step - loss: 1.8459 - sparse_categorical_accuracy: 0.3049\n",
      "Epoch 9/10\n",
      "189/189 [==============================] - 11s 56ms/step - loss: 1.8430 - sparse_categorical_accuracy: 0.3067\n",
      "Epoch 10/10\n",
      "189/189 [==============================] - 11s 56ms/step - loss: 1.8374 - sparse_categorical_accuracy: 0.3115\n",
      "313/313 [==============================] - 18s 12ms/step - loss: 1.7339 - sparse_categorical_accuracy: 0.3528\n",
      "--- Starting trial: run-6\n",
      "{'temperature': 0.12355834498251328, 'optimizer': 'SGD', 'learning_rate': 0.15799313996496078}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "189/189 - 38s - loss: 5.5081 - 38s/epoch - 198ms/step\n",
      "Epoch 1/10\n",
      "189/189 [==============================] - 18s 59ms/step - loss: 1.6732 - sparse_categorical_accuracy: 0.4103\n",
      "Epoch 2/10\n",
      "189/189 [==============================] - 11s 56ms/step - loss: 1.5488 - sparse_categorical_accuracy: 0.4414\n",
      "Epoch 3/10\n",
      "189/189 [==============================] - 10s 56ms/step - loss: 1.5126 - sparse_categorical_accuracy: 0.4506\n",
      "Epoch 4/10\n",
      "189/189 [==============================] - 11s 57ms/step - loss: 1.4825 - sparse_categorical_accuracy: 0.4560\n",
      "Epoch 5/10\n",
      "189/189 [==============================] - 10s 55ms/step - loss: 1.4837 - sparse_categorical_accuracy: 0.4587\n",
      "Epoch 6/10\n",
      "189/189 [==============================] - 10s 55ms/step - loss: 1.4676 - sparse_categorical_accuracy: 0.4630\n",
      "Epoch 7/10\n",
      "189/189 [==============================] - 11s 56ms/step - loss: 1.4635 - sparse_categorical_accuracy: 0.4656\n",
      "Epoch 8/10\n",
      "189/189 [==============================] - 10s 55ms/step - loss: 1.4514 - sparse_categorical_accuracy: 0.4705\n",
      "Epoch 9/10\n",
      "189/189 [==============================] - 11s 58ms/step - loss: 1.4546 - sparse_categorical_accuracy: 0.4681\n",
      "Epoch 10/10\n",
      "189/189 [==============================] - 11s 56ms/step - loss: 1.4446 - sparse_categorical_accuracy: 0.4718\n",
      "313/313 [==============================] - 18s 12ms/step - loss: 1.3710 - sparse_categorical_accuracy: 0.4942\n",
      "--- Starting trial: run-7\n",
      "{'temperature': 0.014626659456859606, 'optimizer': 'adam', 'learning_rate': 0.0013602300641906559}\n",
      "189/189 - 57s - loss: 10.3248 - 57s/epoch - 300ms/step\n",
      "Epoch 1/10\n",
      "189/189 [==============================] - 19s 64ms/step - loss: 4.7598 - sparse_categorical_accuracy: 0.1501\n",
      "Epoch 2/10\n",
      "189/189 [==============================] - 12s 63ms/step - loss: 2.2163 - sparse_categorical_accuracy: 0.1701\n",
      "Epoch 3/10\n",
      "189/189 [==============================] - 11s 58ms/step - loss: 2.1963 - sparse_categorical_accuracy: 0.1739\n",
      "Epoch 4/10\n",
      "189/189 [==============================] - 11s 58ms/step - loss: 2.1844 - sparse_categorical_accuracy: 0.1744\n",
      "Epoch 5/10\n",
      "189/189 [==============================] - 11s 58ms/step - loss: 2.1764 - sparse_categorical_accuracy: 0.1779\n",
      "Epoch 6/10\n",
      "189/189 [==============================] - 22s 117ms/step - loss: 2.1736 - sparse_categorical_accuracy: 0.1773\n",
      "Epoch 7/10\n",
      "189/189 [==============================] - 11s 56ms/step - loss: 2.1697 - sparse_categorical_accuracy: 0.1766\n",
      "313/313 [==============================] - 17s 12ms/step - loss: 2.1489 - sparse_categorical_accuracy: 0.1943\n",
      "--- Starting trial: run-8\n",
      "{'temperature': 0.01960442658292879, 'optimizer': 'SGD', 'learning_rate': 0.4194842499443589}\n",
      "189/189 - 38s - loss: 7.3366 - 38s/epoch - 203ms/step\n",
      "Epoch 1/10\n",
      "189/189 [==============================] - 16s 56ms/step - loss: 2.8758 - sparse_categorical_accuracy: 0.1222\n",
      "Epoch 2/10\n",
      "189/189 [==============================] - 11s 56ms/step - loss: 2.2950 - sparse_categorical_accuracy: 0.1284\n",
      "Epoch 3/10\n",
      "189/189 [==============================] - 11s 56ms/step - loss: 2.2829 - sparse_categorical_accuracy: 0.1289\n",
      "Epoch 4/10\n",
      "189/189 [==============================] - 11s 56ms/step - loss: 2.2740 - sparse_categorical_accuracy: 0.1420\n",
      "Epoch 5/10\n",
      "189/189 [==============================] - 10s 55ms/step - loss: 2.2713 - sparse_categorical_accuracy: 0.1403\n",
      "Epoch 6/10\n",
      "189/189 [==============================] - 12s 63ms/step - loss: 2.2664 - sparse_categorical_accuracy: 0.1400\n",
      "313/313 [==============================] - 18s 12ms/step - loss: 2.2454 - sparse_categorical_accuracy: 0.1545\n",
      "--- Starting trial: run-9\n",
      "{'temperature': 0.14383485948403216, 'optimizer': 'adam', 'learning_rate': 0.1557032048305991}\n",
      "189/189 - 40s - loss: 5.5372 - 40s/epoch - 212ms/step\n",
      "Epoch 1/10\n",
      "189/189 [==============================] - 17s 56ms/step - loss: 2.4007 - sparse_categorical_accuracy: 0.1691\n",
      "Epoch 2/10\n",
      "189/189 [==============================] - 11s 57ms/step - loss: 2.0809 - sparse_categorical_accuracy: 0.1855\n",
      "Epoch 3/10\n",
      "189/189 [==============================] - 11s 56ms/step - loss: 2.0676 - sparse_categorical_accuracy: 0.1881\n",
      "Epoch 4/10\n",
      "189/189 [==============================] - 11s 57ms/step - loss: 2.0605 - sparse_categorical_accuracy: 0.1866\n",
      "Epoch 5/10\n",
      "189/189 [==============================] - 10s 55ms/step - loss: 2.0548 - sparse_categorical_accuracy: 0.1934\n",
      "Epoch 6/10\n",
      "189/189 [==============================] - 10s 55ms/step - loss: 2.0560 - sparse_categorical_accuracy: 0.1931\n",
      "Epoch 7/10\n",
      "189/189 [==============================] - 11s 56ms/step - loss: 2.0547 - sparse_categorical_accuracy: 0.1937\n",
      "Epoch 8/10\n",
      "189/189 [==============================] - 11s 57ms/step - loss: 2.0547 - sparse_categorical_accuracy: 0.1952\n",
      "Epoch 9/10\n",
      "189/189 [==============================] - 10s 55ms/step - loss: 2.0536 - sparse_categorical_accuracy: 0.1940\n",
      "Epoch 10/10\n",
      "189/189 [==============================] - 11s 59ms/step - loss: 2.0592 - sparse_categorical_accuracy: 0.1918\n",
      "313/313 [==============================] - 23s 13ms/step - loss: 2.0014 - sparse_categorical_accuracy: 0.2056\n"
     ]
    }
   ],
   "source": [
    "hparams_resnet={\"depth_first_convolution\":64,\n",
    "                \"output_dim\":2048,\n",
    "                \"number_of_block\":5,\n",
    "                \"downsample_num\":[4,5],\n",
    "                \"conv_by_block\":2,\n",
    "                \"globalPoolingType\":\"Mean\"\n",
    "            }\n",
    "\n",
    "\n",
    "num_epochs=1\n",
    "\n",
    "def train_test_model(hparams):\n",
    "    \n",
    "    \n",
    "    learning_rate=hparams[\"learning_rate\"]\n",
    "    \n",
    "    optimizer= keras.optimizers.Adam(learning_rate) if hparams[\"optimizer\"]==\"adam\" else keras.optimizers.SGD(learning_rate)\n",
    "    temperature=hparams[\"temperature\"]\n",
    "    \n",
    "    resnet=get_resnet_backbone(input_shape,hparams_resnet)\n",
    "    \n",
    "    \n",
    "    #first step : contrastive loss\n",
    "    early_stop_unsup=tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"loss\", patience=2, restore_best_weights=True\n",
    "    )\n",
    "    \n",
    "    resnet=get_resnet_backbone(input_shape,hparams_resnet)\n",
    "\n",
    "    encoder = create_encoder(resnet)\n",
    "\n",
    "    encoder_with_projection_head = add_projection_head(encoder)\n",
    "    encoder_with_projection_head.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=SupervisedContrastiveLoss(temperature),\n",
    "    )\n",
    "\n",
    " \n",
    "\n",
    "    history = encoder_with_projection_head.fit(\n",
    "        x=x_train, y=y_train, batch_size=batch_size, epochs=num_epochs,callbacks=[early_stop_unsup],verbose=2\n",
    "    )\n",
    "\n",
    "    \n",
    "    #second step : linear classifier\n",
    "    \n",
    "    early_stop=tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"sparse_categorical_accuracy\", patience=2, restore_best_weights=True\n",
    "    )\n",
    "\n",
    "\n",
    "    classifier = create_classifier(encoder,trainable=False)\n",
    "\n",
    "    history = classifier.fit(x=x_train, y=y_train, batch_size=batch_size, epochs=10,callbacks=[early_stop])\n",
    "\n",
    "    \n",
    "    accuracy = classifier.evaluate(x_test, y_test)[1]\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def run(run_dir, hparams):\n",
    "  with tf.summary.create_file_writer(run_dir).as_default():\n",
    "    hp.hparams(hparams)  # record the values used in this trial\n",
    "    accuracy = train_test_model(hparams)\n",
    "    tf.summary.scalar(\"accuracy\", accuracy, step=1)\n",
    "\n",
    "\n",
    "\n",
    "   \n",
    "\n",
    " \n",
    "\n",
    "for session_num,hparams in config_to_test.items():\n",
    "    run_name = \"run-%d\" % session_num\n",
    "    print('--- Starting trial: %s' % run_name)\n",
    "    print(hparams)\n",
    "    run('logs_resnet/hparam_tuning/' + run_name, hparams)\n",
    "    \n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
