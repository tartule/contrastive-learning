{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "660920ae",
   "metadata": {},
   "source": [
    "# implementation using the cross_entropy loss (such as in n_pairs_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39fa23f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "51b87d67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contcatenation 0.0\n",
      "similarity computation 0.0\n",
      "label computation 0.0\n",
      "mask computation 0.01604318618774414\n",
      "numerical stability computation 0.0\n",
      "loss computation 0.0\n",
      "contcatenation 0.0\n",
      "similarity computation 0.0\n",
      "label computation 0.03124690055847168\n",
      "mask computation 0.0\n",
      "numerical stability computation 0.0\n",
      "loss computation 0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6.859067013451332e-11"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "class SupervisedContrastiveLoss(keras.losses.Loss):\n",
    "    def __init__(self, temperature=1, name=None):\n",
    "        super(SupervisedContrastiveLoss, self).__init__(reduction=tf.keras.losses.Reduction.NONE,name=name)\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def __call__(self, y_true, feature_vectors1,feature_vectors2, sample_weight=None):\n",
    "        #adapted from https://github.com/HobbitLong/SupContrast/blob/master/losses.py\n",
    "        #only change should be the change of computation of the maximum for the numerical stability trick\n",
    "        \n",
    "        \n",
    "        #feature vectors1_2 : shape (batch_size,_)\n",
    "        \n",
    "        t=time.time()\n",
    "        feature_vectors=tf.concat([feature_vectors1,feature_vectors2],axis=0)\n",
    "        # Normalize feature vectors\n",
    "        print(\"contcatenation\",time.time()-t)\n",
    "        t=time.time()\n",
    "        feature_vectors_normalized = tf.math.l2_normalize(feature_vectors, axis=1)\n",
    "        # Compute logits\n",
    "        \n",
    "        \n",
    "        logits = tf.divide(\n",
    "            tf.matmul(\n",
    "                feature_vectors_normalized, tf.transpose(feature_vectors_normalized)\n",
    "            ),\n",
    "            self.temperature,\n",
    "        )\n",
    "        \n",
    "        print(\"similarity computation\",time.time()-t)\n",
    "        t=time.time()\n",
    "        #y_pred = tf.convert_to_tensor(logits)\n",
    "        #print(y_pred.shape)\n",
    "        y_true = tf.cast(y_true, logits.dtype)\n",
    "\n",
    "        # Expand to [2*batch_size, 1]\n",
    "        y_true=tf.concat([y_true,y_true],axis=0)\n",
    "        \n",
    "        #y_true = tf.expand_dims(y_true, -1)\n",
    "        is_similar = tf.cast(tf.equal(y_true, tf.transpose(y_true)), logits.dtype)\n",
    "        \n",
    "        print(\"label computation\",time.time()-t)\n",
    "        t=time.time()\n",
    "        \n",
    "        \n",
    "        #mask for self_contrasting terms\n",
    "        bsz=y_true.shape[0]\n",
    "        indices=[[i,i] for i in range(bsz)]\n",
    "        update=[0 for i in range(bsz)]\n",
    "        mask_logits=tf.ones_like(logits)\n",
    "        mask_logits=tf.tensor_scatter_nd_update(mask_logits,indices,update)\n",
    "        is_similar=is_similar*mask_logits\n",
    "        \n",
    "        print(\"mask computation\",time.time()-t)\n",
    "        t=time.time()\n",
    "        #for numerical stability \n",
    "        \n",
    "        max_logits=tf.math.reduce_max(mask_logits*logits,axis=1,keepdims=True)\n",
    "        \n",
    "        logits_unstabalize=logits\n",
    "        logits=logits-tf.stop_gradient(max_logits)\n",
    "        \n",
    "        \n",
    "        print(\"numerical stability computation\",time.time()-t)\n",
    "        t=time.time()\n",
    "        \n",
    "        log_softmax=logits-tf.math.log(tf.reduce_sum(mask_logits*tf.math.exp(logits),axis=1,keepdims=True))\n",
    "        \n",
    "        loss=tf.reduce_sum(is_similar*log_softmax,axis=1)/tf.reduce_sum(is_similar,axis=1)#compute the mean of the log likelhood\n",
    "        loss=-tf.math.reduce_mean(loss)\n",
    "        \n",
    "        print(\"loss computation\",time.time()-t)\n",
    "      \n",
    "        \n",
    "        \n",
    "        return loss \n",
    "\n",
    "    \n",
    "class SupervisedContrastiveLoss2(keras.losses.Loss):\n",
    "    def __init__(self, temperature=1, name=None):\n",
    "        super(SupervisedContrastiveLoss2, self).__init__(reduction=tf.keras.losses.Reduction.NONE,name=name)\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def __call__(self, y_true, feature_vectors1,feature_vectors2, sample_weight=None):\n",
    "        #adapted from https://github.com/HobbitLong/SupContrast/blob/master/losses.py\n",
    "        #only change should be the change of computation of the maximum for the numerical stability trick\n",
    "        \n",
    "        \n",
    "        #feature vectors1_2 : shape (batch_size,_)\n",
    "        \n",
    "        t=time.time()\n",
    "        feature_vectors=tf.concat([feature_vectors1,feature_vectors2],axis=0)\n",
    "        # Normalize feature vectors\n",
    "        print(\"contcatenation\",time.time()-t)\n",
    "        t=time.time()\n",
    "        feature_vectors_normalized = tf.math.l2_normalize(feature_vectors, axis=1)\n",
    "        # Compute logits\n",
    "        \n",
    "        \n",
    "        logits = tf.divide(\n",
    "            tf.matmul(\n",
    "                feature_vectors_normalized, tf.transpose(feature_vectors_normalized)\n",
    "            ),\n",
    "            self.temperature,\n",
    "        )\n",
    "        \n",
    "        print(\"similarity computation\",time.time()-t)\n",
    "        t=time.time()\n",
    "        #y_pred = tf.convert_to_tensor(logits)\n",
    "        #print(y_pred.shape)\n",
    "        y_true = tf.cast(y_true, logits.dtype)\n",
    "\n",
    "        # Expand to [2*batch_size, 1]\n",
    "        y_true=tf.concat([y_true,y_true],axis=0)\n",
    "        \n",
    "        #y_true = tf.expand_dims(y_true, -1)\n",
    "        is_similar = tf.cast(tf.equal(y_true, tf.transpose(y_true)), logits.dtype)\n",
    "        \n",
    "        print(\"label computation\",time.time()-t)\n",
    "        t=time.time()\n",
    "        \n",
    "        \n",
    "        #mask for self_contrasting terms\n",
    "        bsz=y_true.shape[0]\n",
    "        indices=[[i,i] for i in range(bsz)]\n",
    "        update=[0 for i in range(bsz)]\n",
    "        mask_logits=tf.ones_like(logits)\n",
    "        mask_logits=tf.tensor_scatter_nd_update(mask_logits,indices,update)\n",
    "        is_similar=is_similar*mask_logits\n",
    "        \n",
    "        print(\"mask computation\",time.time()-t)\n",
    "        t=time.time()\n",
    "        #for numerical stability \n",
    "        \n",
    "        max_logits=tf.math.reduce_max(logits,axis=1,keepdims=True)\n",
    "        \n",
    "        logits_unstabalize=logits\n",
    "        logits=logits-tf.stop_gradient(max_logits)\n",
    "        \n",
    "        \n",
    "        print(\"numerical stability computation\",time.time()-t)\n",
    "        t=time.time()\n",
    "        \n",
    "        log_softmax=logits-tf.math.log(tf.reduce_sum(mask_logits*tf.math.exp(logits),axis=1,keepdims=True))\n",
    "        \n",
    "        loss=tf.reduce_sum(is_similar*log_softmax,axis=1)/tf.reduce_sum(is_similar,axis=1)#compute the mean of the log likelhood\n",
    "        loss=-tf.math.reduce_mean(loss)\n",
    "        \n",
    "        print(\"loss computation\",time.time()-t)\n",
    "      \n",
    "        \n",
    "        \n",
    "        return loss \n",
    "               \n",
    "size=128\n",
    "\n",
    "\n",
    "\n",
    "temperature=0.1\n",
    "contrastive_loss=SupervisedContrastiveLoss(temperature=temperature)\n",
    "contrastive_loss2=SupervisedContrastiveLoss2(temperature=temperature)\n",
    "\n",
    "\n",
    "\n",
    "embeding1=tf.convert_to_tensor(np.random.randint(0,2,(size,1048)),dtype=tf.float64)\n",
    "embeding2=tf.convert_to_tensor(np.random.randint(0,2,(size,1048)),dtype=tf.float64)\n",
    "y_true=tf.convert_to_tensor(np.random.randint(0,2,(size)))\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(embeding1)\n",
    "    \n",
    "    loss=contrastive_loss(y_true,embeding1,embeding2)\n",
    "\n",
    "gradients = tape.gradient(loss, embeding1)\n",
    "gradients\n",
    "\n",
    "\n",
    "y_true=tf.convert_to_tensor(np.random.randint(0,2,(size)))\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(embeding1)\n",
    "    \n",
    "    loss=contrastive_loss2(y_true,embeding1,embeding2)\n",
    "\n",
    "gradients2 = tape.gradient(loss, embeding1)\n",
    "np.max(np.abs(((gradients2-gradients)/gradients).numpy()))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
