{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32b7a181",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install tensorflow-addons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c020e47d",
   "metadata": {},
   "source": [
    "I want to optimize the following parameters :\n",
    "\n",
    "\n",
    "- learning rate\n",
    "- optimizer (SGD,adam)\n",
    "- temperature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6916bc",
   "metadata": {},
   "source": [
    "# set up the parameters needed for the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "491671ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorboard.plugins.hparams import api as hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9d5ee93",
   "metadata": {},
   "outputs": [],
   "source": [
    "HP_temperature = hp.HParam('temperature', hp.RealInterval(0.,0.2))\n",
    "HP_learning_rate = hp.HParam('learning_rate', hp.RealInterval(0., 0.2))\n",
    "HP_OPTIMIZER = hp.HParam('optimizer', hp.Discrete(['adam', 'sgd']))\n",
    "\n",
    "METRIC_ACCURACY = 'accuracy'\n",
    "\n",
    "\n",
    "with tf.summary.create_file_writer('logs/hparam_tuning').as_default():\n",
    "  hp.hparams_config(\n",
    "    hparams=[HP_temperature, HP_learning_rate, HP_OPTIMIZER],\n",
    "    metrics=[hp.Metric(METRIC_ACCURACY, display_name='Accuracy')],\n",
    "  )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0efa49c",
   "metadata": {},
   "source": [
    "# define the training of the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7797c2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "100dbd81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (50000, 32, 32, 3) - y_train shape: (50000, 1)\n",
      "x_test shape: (10000, 32, 32, 3) - y_test shape: (10000, 1)\n"
     ]
    }
   ],
   "source": [
    "num_classes = 10\n",
    "input_shape = (32, 32, 3)\n",
    "\n",
    "AUTO=tf.data.AUTOTUNE\n",
    "#learning_rate = 0.001\n",
    "batch_size = 265\n",
    "hidden_units = 512\n",
    "projection_units = 128\n",
    "num_epochs = 50\n",
    "dropout_rate = 0.5\n",
    "#temperature = 0.05\n",
    "\n",
    "\n",
    "\n",
    "# Load the train and test data splits\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "# Display shapes of train and test datasets\n",
    "print(f\"x_train shape: {x_train.shape} - y_train shape: {y_train.shape}\")\n",
    "print(f\"x_test shape: {x_test.shape} - y_test shape: {y_test.shape}\")\n",
    "\n",
    "test_dataset=(tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "    .shuffle(1024)\n",
    "    .batch(batch_size)\n",
    "    .prefetch(AUTO)\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f4ca8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = keras.Sequential(\n",
    "    [\n",
    "        layers.Normalization(),\n",
    "        layers.RandomFlip(\"horizontal\"),\n",
    "        layers.RandomRotation(0.02),\n",
    "        layers.RandomWidth(0.2),\n",
    "        layers.RandomHeight(0.2),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Setting the state of the normalization layer.\n",
    "data_augmentation.layers[0].adapt(x_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "272493ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_encoder(backbone=keras.applications.ResNet50V2(\n",
    "        include_top=False, weights=None, input_shape=input_shape, pooling=\"avg\"\n",
    "    )):\n",
    "    \n",
    "\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    augmented = data_augmentation(inputs)\n",
    "    outputs = backbone(augmented)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs, name=\"cifar10-encoder\")\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_classifier(encoder, trainable=True):\n",
    "    learning_rate=0.001\n",
    "    for layer in encoder.layers:\n",
    "        layer.trainable = trainable\n",
    "\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    features = encoder(inputs)\n",
    "    features = layers.Dropout(dropout_rate)(features)\n",
    "    features = layers.Dense(hidden_units, activation=\"relu\")(features)\n",
    "    features = layers.Dropout(dropout_rate)(features)\n",
    "    outputs = layers.Dense(num_classes, activation=\"softmax\")(features)\n",
    "\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs, name=\"cifar10-classifier\")\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate),\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "        metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "class SupervisedContrastiveLoss(keras.losses.Loss):\n",
    "    def __init__(self, temperature=1, name=None):\n",
    "        super(SupervisedContrastiveLoss, self).__init__(name=name)\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def __call__(self, labels, feature_vectors, sample_weight=None):\n",
    "        # Normalize feature vectors\n",
    "        feature_vectors_normalized = tf.math.l2_normalize(feature_vectors, axis=1)\n",
    "        # Compute logits\n",
    "        logits = tf.divide(\n",
    "            tf.matmul(\n",
    "                feature_vectors_normalized, tf.transpose(feature_vectors_normalized)\n",
    "            ),\n",
    "            self.temperature,\n",
    "        )\n",
    "        return tfa.losses.npairs_loss(tf.squeeze(labels), logits)\n",
    "\n",
    "\n",
    "def add_projection_head(encoder):\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    features = encoder(inputs)\n",
    "    outputs = layers.Dense(projection_units, activation=\"relu\")(features)\n",
    "    model = keras.Model(\n",
    "        inputs=inputs, outputs=outputs, name=\"cifar-encoder_with_projection-head\"\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def apply_resnet_block(x,downsample,conv_by_block):\n",
    "    \n",
    "    depth_input=x.shape[-1]\n",
    "    \n",
    "    \n",
    "    if downsample:\n",
    "        depth=depth_input*2\n",
    "        skiped=layers.Conv2D(depth,1,strides=(2,2),activation=None)(x)#linear projection\n",
    "        x=layers.Conv2D(depth,3,strides=(2,2), activation='relu',padding=\"same\")(x)\n",
    "        x=layers.BatchNormalization()(x)\n",
    "    else:\n",
    "        depth=depth_input\n",
    "        skiped=x\n",
    "        x=layers.Conv2D(depth,3, activation='relu',padding=\"same\")(x)\n",
    "        x=layers.BatchNormalization()(x)\n",
    "        \n",
    "    for i in range(1,conv_by_block-1):\n",
    "        x=layers.Conv2D(depth,3, activation='relu',padding=\"same\")(x)\n",
    "        x=layers.BatchNormalization()(x)\n",
    "    \n",
    "    x=layers.Conv2D(depth,3,padding=\"same\")(x)#don't apply activation to the last \n",
    "         \n",
    "    x=skiped+x\n",
    "    x=layers.ReLU()(x)\n",
    "    x=layers.BatchNormalization()(x)\n",
    "   \n",
    "    return x\n",
    "\n",
    "def get_resnet_backbone(input_shape,hparams):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    assert hparams[\"conv_by_block\"]>=2\n",
    "    inputs=layers.Input((32,32,3))\n",
    "    x=layers.Conv2D(hparams[\"depth_first_convolution\"],7,strides=(2,2),activation='relu',padding=\"same\")(inputs)\n",
    "    x=layers.BatchNormalization()(x)\n",
    "    for block in range(1,hparams[\"number_of_block\"]+1):\n",
    "        x=apply_resnet_block(x,block in hparams[\"downsample_num\"],hparams[\"conv_by_block\"])\n",
    "       \n",
    "        \n",
    "    x=layers.Conv2D(hparams[\"output_dim\"],3, activation='relu',padding=\"same\")(x)\n",
    "    x=layers.BatchNormalization()(x)\n",
    "    if hparams[\"globalPoolingType\"]==\"Mean\":\n",
    "        \n",
    "        x=layers.GlobalAveragePooling2D()(x)\n",
    " \n",
    "    return tf.keras.Model(inputs,x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb95fd8c",
   "metadata": {},
   "source": [
    "## model definition with the api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60b2267f",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_training=10\n",
    "learning_rate=10**np.random.uniform(-0.3,-3,number_of_training)\n",
    "temperature=np.random.uniform(0.1,0.2,number_of_training)\n",
    "optimizer=np.random.choice([\"adam\",\"SGD\"],number_of_training)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fcc0ff35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 {'temperature': 0.13231649284801916, 'optimizer': 'adam', 'learning_rate': 0.4219266037489388}\n",
      "1 {'temperature': 0.11507286220513029, 'optimizer': 'SGD', 'learning_rate': 0.0026480865708758904}\n",
      "2 {'temperature': 0.11446795344445078, 'optimizer': 'adam', 'learning_rate': 0.1282556862721409}\n",
      "3 {'temperature': 0.19280501278030793, 'optimizer': 'adam', 'learning_rate': 0.2408232548776279}\n",
      "4 {'temperature': 0.14370969326059763, 'optimizer': 'adam', 'learning_rate': 0.0021795933073952105}\n",
      "5 {'temperature': 0.10932611261569408, 'optimizer': 'adam', 'learning_rate': 0.0034401292377040722}\n",
      "6 {'temperature': 0.1618644820572037, 'optimizer': 'adam', 'learning_rate': 0.004932435789081691}\n",
      "7 {'temperature': 0.15665449778977641, 'optimizer': 'SGD', 'learning_rate': 0.1435130983509785}\n",
      "8 {'temperature': 0.15311862435684215, 'optimizer': 'adam', 'learning_rate': 0.0033010566433369156}\n",
      "9 {'temperature': 0.14299964039368063, 'optimizer': 'SGD', 'learning_rate': 0.01750078415979348}\n"
     ]
    }
   ],
   "source": [
    "config_to_test={i:{\"temperature\":temperature[i],\"optimizer\":optimizer[i],\"learning_rate\":learning_rate[i]} for i in range(number_of_training)}\n",
    "for n,hparams in config_to_test.items():\n",
    "    print(n,hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dccc740",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.log10(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae8919d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting trial: run-0\n",
      "{'temperature': 0.13231649284801916, 'optimizer': 'adam', 'learning_rate': 0.4219266037489388}\n",
      "Epoch 1/50\n",
      "189/189 - 96s - loss: 5.6022 - 96s/epoch - 509ms/step\n",
      "Epoch 2/50\n",
      "189/189 - 36s - loss: 5.4592 - 36s/epoch - 189ms/step\n",
      "Epoch 3/50\n",
      "189/189 - 34s - loss: 5.3519 - 34s/epoch - 179ms/step\n",
      "Epoch 4/50\n",
      "189/189 - 33s - loss: 5.3277 - 33s/epoch - 174ms/step\n",
      "Epoch 5/50\n",
      "189/189 - 31s - loss: 5.2980 - 31s/epoch - 164ms/step\n",
      "Epoch 6/50\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_22444\\2985736522.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     76\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'--- Starting trial: %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mrun_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m     \u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'logs_resnet/hparam_tuning/'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mrun_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     79\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_22444\\2985736522.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(run_dir, hparams)\u001b[0m\n\u001b[0;32m     63\u001b[0m   \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_file_writer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[0mhp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhparams\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhparams\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# record the values used in this trial\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m     \u001b[0maccuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m     \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"accuracy\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_22444\\2985736522.py\u001b[0m in \u001b[0;36mtrain_test_model\u001b[1;34m(hparams)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m     history = encoder_with_projection_head.fit(\n\u001b[1;32m---> 41\u001b[1;33m         \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mearly_stop_unsup\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m     )\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1374\u001b[0m         \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1375\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcatch_stop_iteration\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1376\u001b[1;33m           \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1377\u001b[0m             with tf.profiler.experimental.Trace(\n\u001b[0;32m   1378\u001b[0m                 \u001b[1;34m'train'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36msteps\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1244\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_insufficient_data\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Set by `catch_stop_iteration`.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1245\u001b[0m         \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1246\u001b[1;33m       \u001b[0moriginal_spe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_steps_per_execution\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1247\u001b[0m       can_run_full_execution = (\n\u001b[0;32m   1248\u001b[0m           \u001b[0moriginal_spe\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    672\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    673\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 674\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    675\u001b[0m     raise NotImplementedError(\n\u001b[0;32m    676\u001b[0m         \"numpy() is only available when eager execution is enabled.\")\n",
      "\u001b[1;32m~\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1221\u001b[0m     \"\"\"\n\u001b[0;32m   1222\u001b[0m     \u001b[1;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1223\u001b[1;33m     \u001b[0mmaybe_arr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1224\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1225\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1187\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1188\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1189\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1190\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1191\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hparams_resnet={\"depth_first_convolution\":64,\n",
    "                \"output_dim\":2048,\n",
    "                \"number_of_block\":5,\n",
    "                \"downsample_num\":[4,5],\n",
    "                \"conv_by_block\":2,\n",
    "                \"globalPoolingType\":\"Mean\"\n",
    "            }\n",
    "\n",
    "\n",
    "num_epochs=1\n",
    "\n",
    "def train_test_model(hparams):\n",
    "    \n",
    "    \n",
    "    learning_rate=hparams[\"learning_rate\"]\n",
    "    \n",
    "    optimizer= keras.optimizers.Adam(learning_rate) if hparams[\"optimizer\"]==\"adam\" else keras.optimizers.SGD(learning_rate)\n",
    "    temperature=hparams[\"temperature\"]\n",
    "    \n",
    "    resnet=get_resnet_backbone(input_shape,hparams_resnet)\n",
    "    \n",
    "    \n",
    "    #first step : contrastive loss\n",
    "    early_stop_unsup=tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"loss\", patience=2, restore_best_weights=True\n",
    "    )\n",
    "    \n",
    "    resnet=get_resnet_backbone(input_shape,hparams_resnet)\n",
    "\n",
    "    encoder = create_encoder(resnet)\n",
    "\n",
    "    encoder_with_projection_head = add_projection_head(encoder)\n",
    "    encoder_with_projection_head.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=SupervisedContrastiveLoss(temperature),\n",
    "    )\n",
    "\n",
    " \n",
    "\n",
    "    history = encoder_with_projection_head.fit(\n",
    "        x=x_train, y=y_train, batch_size=batch_size, epochs=num_epochs,callbacks=[early_stop_unsup],verbose=2\n",
    "    )\n",
    "\n",
    "    \n",
    "    #second step : linear classifier\n",
    "    \n",
    "    early_stop=tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"sparse_categorical_accuracy\", patience=2, restore_best_weights=True\n",
    "    )\n",
    "\n",
    "\n",
    "    classifier = create_classifier(encoder,trainable=False)\n",
    "\n",
    "    history = classifier.fit(x=x_train, y=y_train, batch_size=batch_size, epochs=10,callbacks=[early_stop])\n",
    "\n",
    "    \n",
    "    accuracy = classifier.evaluate(x_test, y_test)[1]\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def run(run_dir, hparams):\n",
    "  with tf.summary.create_file_writer(run_dir).as_default():\n",
    "    hp.hparams(hparams)  # record the values used in this trial\n",
    "    accuracy = train_test_model(hparams)\n",
    "    tf.summary.scalar(\"accuracy\", accuracy, step=1)\n",
    "\n",
    "\n",
    "\n",
    "   \n",
    "\n",
    " \n",
    "\n",
    "for session_num,hparams in config_to_test.items():\n",
    "    run_name = \"run-%d\" % session_num\n",
    "    print('--- Starting trial: %s' % run_name)\n",
    "    print(hparams)\n",
    "    run('logs_resnet/hparam_tuning/' + run_name, hparams)\n",
    "    \n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
